{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "from twython import Twython  \n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import gensim \n",
    "import re, string\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import re\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_tweets(data_dir, auth, time_limit, topic):\n",
    "    file_name = \"stream\"\n",
    "    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')\n",
    "    twitter_stream.filter(track= topic) # list of querries to track\n",
    "    data = pd.read_json(data_dir+file_name+\".json\",lines=True)\n",
    "    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)\n",
    "    no_data = False\n",
    "    if data.empty:\n",
    "        no_data = True\n",
    "    else:\n",
    "        data = data[data.lang=='en']\n",
    "        #To get the full-text of the tweet\n",
    "        for i in data.index:  \n",
    "               tweets.time[i] = data.created_at[i]\n",
    "               if pd.isnull(data.retweeted_status[i]):\n",
    "                     if pd.isnull(data.extended_tweet[i]):\n",
    "                            tweets.tweet[i] = data.text[i]\n",
    "                     else:   \n",
    "                        if \"full_text\" in data.extended_tweet[i].keys():\n",
    "                             tweets.tweet[i]=data.extended_tweet[i][\"full_text\"]\n",
    "\n",
    "                        else:\n",
    "                             tweets.tweet[i]=data.text[i] \n",
    "               else:\n",
    "                    if 'extended_tweet' in data.retweeted_status[i].keys():\n",
    "                        if \"full_text\" in data.retweeted_status[i]['extended_tweet'].keys():\n",
    "                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet'][\"full_text\"]\n",
    "                    else:\n",
    "                         tweets.tweet[i] = data.retweeted_status[i]['text']     \n",
    "        tweets = tweets.sort_values('time', ascending=False)\n",
    "        tweets=tweets.drop_duplicates()\n",
    "        tweets.dropna(subset=['tweet'])\n",
    "    return tweets, no_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MyListener() saves the data into a .json file with name stream_query\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query, time_limit=60):\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        #query_fname = format_filename(query)\n",
    "        self.saveFile = open(data_dir+\"stream.json\", 'a')\n",
    "        super(MyListener, self).__init__()\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if (time.time() - self.start_time) < self.limit:\n",
    "            self.saveFile.write(data)\n",
    "            return True\n",
    "        else:\n",
    "            self.saveFile.close()\n",
    "            return False\n",
    "            \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "class TwitterPreprocessor:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "    \n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')  \n",
    "    \n",
    "    def fully_preprocess(self):\n",
    "        return self \\\n",
    "            .remove_hyphens() \\\n",
    "            .remove_urls() \\\n",
    "            .remove_mentions() \\\n",
    "            .remove_hashtags() \\\n",
    "            .remove_twitter_reserved_words() \\\n",
    "            .remove_single_letter_words() \\\n",
    "            .remove_stopwords() \\\n",
    "            .remove_numbers() \\\n",
    "            .strip_html_tags() \\\n",
    "            .remove_accented_chars() \\\n",
    "            .expand_contractions() \\\n",
    "            .lemmatize_text() \\\n",
    "            .remove_special_characters() \\\n",
    "            .remove_stopwords() \\\n",
    "            .remove_blank_spaces() \\\n",
    "            .Lower()\n",
    "    \n",
    "    def Lower(self):\n",
    "        self.text = self.text.lower()\n",
    "        return self\n",
    "    def strip_html_tags(self):\n",
    "        soup = BeautifulSoup(self.text, \"html.parser\")\n",
    "        self.text = soup.get_text()\n",
    "        return self\n",
    "    \n",
    "    def remove_accented_chars(self):\n",
    "        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return self\n",
    "    \n",
    "    def expand_contractions(self):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = CONTRACTION_MAP.get(match) \\\n",
    "                                   if CONTRACTION_MAP.get(match) \\\n",
    "                                    else CONTRACTION_MAP.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "        \n",
    "        expanded_text = contractions_pattern.sub(expand_match, self.text)\n",
    "        self.text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return self\n",
    "    \n",
    "# # Removing Special Characters\n",
    "    def remove_special_characters(self):\n",
    "        self.text = re.sub('[^a-zA-Z0-9\\s]', ' ', self.text)\n",
    "        return self\n",
    "\n",
    "\n",
    "# # Lemmatizing text\n",
    "    def lemmatize_text(self):\n",
    "        nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "        text = nlp(self.text)\n",
    "        self.text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "        return self\n",
    "# # Removing Stopwords\n",
    "    def remove_stopwords(self):\n",
    "        tokens = tokenizer.tokenize(self.text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "        self.text = ' '.join(filtered_tokens)    \n",
    "        return self\n",
    "    \n",
    "    def remove_hyphens(self):\n",
    "        self.text=self.text.replace(\"-\",\" \")\n",
    "        return self\n",
    "    def remove_urls(self):\n",
    "        self.text = re.sub(pattern=re.compile(\n",
    "        r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})'), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "#     def remove_punctuation(self):\n",
    "#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n",
    "#         return self\n",
    " \n",
    "    def remove_mentions(self):\n",
    "        self.text = re.sub(pattern=re.compile(r'@\\w*'), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_hashtags(self):\n",
    "        self.text = re.sub(pattern=re.compile(r'#*'),repl='',string=self.text)\n",
    "        return self\n",
    "    def remove_twitter_reserved_words(self):\n",
    "        self.text = re.sub(r'\\brt\\b','',self.text,flags=re.IGNORECASE) #removing rt\n",
    "        self.text = re.sub(r'\\bvia\\b','',self.text,flags=re.IGNORECASE) #removing via\n",
    "        return self\n",
    "\n",
    "    def remove_single_letter_words(self):\n",
    "        self.text = re.sub(pattern=re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])'),repl=' ',string=self.text)\n",
    "        return self\n",
    "    def remove_stopwords(self, extra_stopwords=None):\n",
    "        if extra_stopwords is None:\n",
    "            extra_stopwords = []\n",
    "        text = nltk.word_tokenize(self.text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        new_sentence = []\n",
    "        for w in text:\n",
    "            if w not in stop_words and w not in extra_stopwords:\n",
    "                new_sentence.append(w)\n",
    "        self.text = ' '.join(new_sentence)\n",
    "        return self\n",
    "    def remove_numbers(self, preserve_years=False):\n",
    "        text_list = self.text.split(' ')\n",
    "        for text in text_list:\n",
    "            if text.isnumeric():\n",
    "                if preserve_years:\n",
    "                    if utils.is_year(text):\n",
    "                        text_list.remove(text)\n",
    "                else:\n",
    "                    text_list.remove(text)\n",
    "\n",
    "        self.text = ' '.join(text_list)\n",
    "        return self\n",
    "    def remove_blank_spaces(self):\n",
    "        self.text = re.sub(pattern=re.compile(r'\\s{2,}|\\t'),repl='',string=self.text)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_latest_tweets(tweets):\n",
    "    tweets.tweet= tweets.tweet.astype(str)\n",
    "    tweets.loc[:,'Normalized'] = tweets.tweet.apply(lambda x:TwitterPreprocessor(x).fully_preprocess().text)\n",
    "    tweets.loc[:,'Normalized'] = [tweet.split() for tweet in tweets.Normalized]\n",
    "    vectorized_tweets = pd.DataFrame(columns=[\"Raw_tweet\",\"Normalized_tweet\",\"Vector\"])\n",
    "\n",
    "    for index, tweet in tweets.iterrows():\n",
    "        word_vecs = [model[word] for word in (tweet.Normalized and model.vocab)]\n",
    "        if len(word_vecs) == 0:\n",
    "            tweet_vec = [np.zeros(300)]\n",
    "        else: \n",
    "            tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))\n",
    "        vectorized_tweets = vectorized_tweets.append \\\n",
    "        ({'Raw_tweet': tweet.tweet, 'Normalized_tweet': tweet.Normalized,'Vector': tweet_vec}, ignore_index=True)\n",
    "    return vectorized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_user_input(user_input):\n",
    "    \n",
    "    Normalized = TwitterPreprocessor(user_input).fully_preprocess().text.split()\n",
    "    word_vecs = [model[word] for word in (Normalized and model.vocab)]\n",
    "    if len(word_vecs) == 0:\n",
    "        tweet_vec = [np.zeros(300)] #300 is the length of the vector model[word]\n",
    "    else:\n",
    "        tweet_vec = preprocessing.normalize(sum(word_vecs).reshape(1, -1))\n",
    "    vectorized_input = {'Raw_input': user_input, 'Normalized': Normalized, 'Vector': tweet_vec}\n",
    "    return vectorized_input\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_tweets(vectorized_input, vectorized_tweets,topn):\n",
    "    vec_tweets=np.vstack(vectorized_tweets.Vector.apply(lambda x: x.tolist()))\n",
    "    cos=model.cosine_similarities(vectorized_input['Vector'].reshape(-1,), vec_tweets)\n",
    "    vectorized_tweets.loc[:,'Similarity_score'] = np.round(cos,10)\n",
    "    decimals = pd.Series([8], index=['Similarity_score'])\n",
    "    vectorized_tweets.round(decimals)\n",
    "    vectorized_tweets = vectorized_tweets.sort_values(by='Similarity_score', ascending=False)\n",
    "    return vectorized_tweets[0:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'brexit' #it can be a list of topics,  comman means 'or'\n",
    "time_limit=20\n",
    "user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'\n",
    "def process_user_input(user_input, time_limit, topic):\n",
    "    data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'\n",
    "    track_list=[k for k in topic.split(',')]\n",
    "    file_name = \"stream\"\n",
    "    if os.path.exists(data_dir+file_name+'.json'):\n",
    "        os.remove(data_dir+file_name+'.json')\n",
    "    #-----------------------------------------------\n",
    "    # Load credentials from json file\n",
    "    with open(\"twitter_credentials.json\", \"r\") as file:  \n",
    "         credentials = json.load(file)\n",
    "\n",
    "    # Instantiate an object\n",
    "    python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])\n",
    "    auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])\n",
    "    auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])\n",
    "    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)\n",
    "    if no_data:\n",
    "        return'There is no data with topic: '+topic+' in the last '+ str(time_limit)\n",
    "    else:\n",
    "#       downloaded pretrained model\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)\n",
    "        vectorized_tweets = vectorize_latest_tweets(latest_tweets)\n",
    "        vectorized_user_input = vectorize_user_input(user_input)\n",
    "#       find the top topn= 10  similar tweets\n",
    "        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)\n",
    "    return recommendations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'brexit' #it can be a list of topics,  comman means 'or'\n",
    "time_limit=20\n",
    "user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'\n",
    "recommendations = process_user_input(user_input, time_limit, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
