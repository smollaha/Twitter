{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "from twython import Twython  \n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import gensim \n",
    "import re, string\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import re\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_tweets(data_dir, auth, time_limit, topic):\n",
    "    file_name = \"stream\"\n",
    "    print(\"streaming tweets...\")\n",
    "    twitter_stream = Stream(auth, MyListener(data_dir, topic, time_limit),tweet_mode='extended')\n",
    "    print(\"tweets obtained and now filtering...\")\n",
    "    twitter_stream.filter(track= topic) # list of querries to track\n",
    "    data = pd.read_json(data_dir+file_name+\".json\",lines=True)\n",
    "    tweets=pd.DataFrame(columns=['time','tweet'],index=data.index)\n",
    "    no_data = False\n",
    "    if data.empty:\n",
    "        no_data = True\n",
    "    else:\n",
    "        data = data[data.lang=='en']\n",
    "        #To get the full-text of the tweet\n",
    "        for i in data.index:  \n",
    "               tweets.time[i] = data.created_at[i]\n",
    "               if pd.isnull(data.retweeted_status[i]):\n",
    "                     if pd.isnull(data.extended_tweet[i]):\n",
    "                            tweets.tweet[i] = data.text[i]\n",
    "                     else:   \n",
    "                        if \"full_text\" in data.extended_tweet[i].keys():\n",
    "                             tweets.tweet[i]=data.extended_tweet[i][\"full_text\"]\n",
    "\n",
    "                        else:\n",
    "                             tweets.tweet[i]=data.text[i] \n",
    "               else:\n",
    "                    if 'extended_tweet' in data.retweeted_status[i].keys():\n",
    "                        if \"full_text\" in data.retweeted_status[i]['extended_tweet'].keys():\n",
    "                            tweets.tweet[i]= data.retweeted_status[i]['extended_tweet'][\"full_text\"]\n",
    "                    else:\n",
    "                         tweets.tweet[i] = data.retweeted_status[i]['text']     \n",
    "        tweets = tweets.sort_values('time', ascending=False)\n",
    "        tweets=tweets.drop_duplicates()\n",
    "        tweets.dropna(subset=['tweet'])\n",
    "    print('tweets are filtered', tweets)\n",
    "    return tweets, no_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MyListener() saves the data into a .json file with name stream_query\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query, time_limit=60):\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        #query_fname = format_filename(query)\n",
    "        self.saveFile = open(data_dir+\"stream.json\", 'a')\n",
    "        super(MyListener, self).__init__()\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if (time.time() - self.start_time) < self.limit:\n",
    "            self.saveFile.write(data)\n",
    "            return True\n",
    "        else:\n",
    "            self.saveFile.close()\n",
    "            return False\n",
    "            \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    " \n",
    "    \n",
    "def fully_preprocess(text):\n",
    "     tic = time.time()\n",
    "     return  \\\n",
    "     remove_hyphens( \\\n",
    "     remove_urls( \\\n",
    "     remove_mentions( \\\n",
    "     remove_hashtags( \\\n",
    "     remove_twitter_reserved_words( \\\n",
    "     remove_single_letter_words( \\\n",
    "     remove_stopwords( \\\n",
    "     remove_numbers(  \\\n",
    "     strip_html_tags(  \\\n",
    "     remove_accented_chars( \\\n",
    "     expand_contractions(  \\\n",
    "     lemmatize_text(  \\\n",
    "     remove_special_characters(  \\\n",
    "     remove_stopwords(  \\\n",
    "     remove_blank_spaces( \\\n",
    "     Lower(text)))))))))))))))), time.time()-tic\n",
    "    \n",
    "def Lower(text):\n",
    "    tic=time.time()\n",
    "    text = text.lower()\n",
    "    toc=time.time()\n",
    "    print('lower'+str(toc-tic))\n",
    "    return text\n",
    "def strip_html_tags(text):\n",
    "    tic=time.time()\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    toc=time.time()\n",
    "    print('str_html'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    tic=time.time()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    toc=time.time()\n",
    "    print('remove accented'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    tic=time.time()\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), \n",
    "                                  flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        tic=time.time()\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = CONTRACTION_MAP.get(match) \\\n",
    "                               if CONTRACTION_MAP.get(match) \\\n",
    "                                else CONTRACTION_MAP.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    text = re.sub(\"'\", \"\", expanded_text)\n",
    "    toc=time.time()\n",
    "    print('expand_contractions'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "# # Removing Special Characters\n",
    "def remove_special_characters(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    toc=time.time()\n",
    "    print('remove special'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "\n",
    "# # Lemmatizing text\n",
    "def lemmatize_text(text):\n",
    "    tic=time.time()\n",
    "    nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    toc=time.time()\n",
    "    print('lemmatize'+str(toc-tic))\n",
    "    return text\n",
    "# # Removing Stopwords\n",
    "#     def remove_stopwords(self):\n",
    "#         tic=time.time()\n",
    "#         tokens = tokenizer.tokenize(self.text)\n",
    "#         tokens = [token.strip() for token in tokens]\n",
    "#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "#         self.text = ' '.join(filtered_tokens) \n",
    "#         toc=time.time()\n",
    "#         print('remove_stop'+str(toc-tic))\n",
    "#         return self\n",
    "def remove_stopwords(text, extra_stopwords=None):\n",
    "    tic=time.time()\n",
    "    if extra_stopwords is None:\n",
    "        extra_stopwords = []\n",
    "    text = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_sentence = []\n",
    "    for w in text:\n",
    "        if w not in stop_words and w not in extra_stopwords:\n",
    "            new_sentence.append(w)\n",
    "    text = ' '.join(new_sentence)\n",
    "    toc=time.time()\n",
    "    print('remove_stop'+str(toc-tic))\n",
    "    return text\n",
    "def remove_hyphens(text):\n",
    "    tic=time.time()\n",
    "    text=text.replace(\"-\",\" \")\n",
    "    toc=time.time()\n",
    "    print('remove_hyphens'+str(toc-tic))\n",
    "    return text\n",
    "def remove_urls(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub(pattern=re.compile(\n",
    "    r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "    r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})'), repl='', string=text)\n",
    "    toc=time.time()\n",
    "    print('remove_urls'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "#     def remove_punctuation(self):\n",
    "#         self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n",
    "#         return self\n",
    "\n",
    "def remove_mentions(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub(pattern=re.compile(r'@\\w*'), repl='', string=text)\n",
    "    toc=time.time()\n",
    "    print('remove_mentions'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub(pattern=re.compile(r'#*'),repl='',string=text)\n",
    "    toc=time.time()\n",
    "    print('remove_hashtags'+str(toc-tic))\n",
    "    return text\n",
    "def remove_twitter_reserved_words(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub(r'\\brt\\b','',text,flags=re.IGNORECASE) #removing rt\n",
    "    text = re.sub(r'\\bvia\\b','',text,flags=re.IGNORECASE) #removing via\n",
    "    toc=time.time()\n",
    "    print('remove_twitter_reserved'+str(toc-tic))\n",
    "    return text\n",
    "\n",
    "def remove_single_letter_words(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub(pattern=re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])'),repl=' ',string=text)\n",
    "    toc=time.time()\n",
    "    print('remove_single_letter'+str(toc-tic))\n",
    "    return text\n",
    "def remove_numbers(text, preserve_years=False):\n",
    "    tic=time.time()\n",
    "    text_list = text.split(' ')\n",
    "    for text in text_list:\n",
    "        if text.isnumeric():\n",
    "            if preserve_years:\n",
    "                if utils.is_year(text):\n",
    "                    text_list.remove(text)\n",
    "            else:\n",
    "                text_list.remove(text)\n",
    "\n",
    "    text = ' '.join(text_list)\n",
    "    toc=time.time()\n",
    "    print('remove_numbers'+str(toc-tic))\n",
    "    return text\n",
    "def remove_blank_spaces(text):\n",
    "    tic=time.time()\n",
    "    text = re.sub(pattern=re.compile(r'\\s{2,}|\\t'),repl='',string=text)\n",
    "    toc=time.time()\n",
    "    print('remove_blank'+str(toc-tic))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower3.0994415283203125e-06\n",
      "remove_blank2.09808349609375e-05\n",
      "remove_stop0.0008819103240966797\n",
      "remove special1.621246337890625e-05\n",
      "lemmatize0.6699390411376953\n",
      "expand_contractions0.00011897087097167969\n",
      "remove accented8.106231689453125e-06\n",
      "str_html0.00023031234741210938\n",
      "remove_numbers6.9141387939453125e-06\n",
      "remove_stop0.0013532638549804688\n",
      "remove_single_letter1.1205673217773438e-05\n",
      "remove_twitter_reserved7.867813110351562e-06\n",
      "remove_hashtags1.0967254638671875e-05\n",
      "remove_mentions7.152557373046875e-06\n",
      "remove_urls6.198883056640625e-06\n",
      "remove_hyphens1.1920928955078125e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('people claim vote', 0.708636999130249)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_preprocess(' The same people who were claiming 80% ‘voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet(tweet):\n",
    "    tokens = [word for word in tweet.lower().split()];\n",
    "    return(tokens);\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    clean_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens];\n",
    "    return(clean_words);\n",
    "\n",
    "def clean_tweet(clean_words):\n",
    "    stoplist = set(stopwords.words('english'));\n",
    "    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];\n",
    "    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];\n",
    "    return(filtered_word_list);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-830832e05690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clean_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclean_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#clean_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-89ca97709184>\u001b[0m in \u001b[0;36mremove_punctuation\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclean_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-89ca97709184>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclean_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "text='49  Labour bigwigs say they now need to commit to a second referendum. Yet in places such as Sunderland it lost 20 per cent of its vote share while the Brexit Party took an extraordinary 40 per cent of the vote. Apparently Labour’s traditional heartlands don’t matter to them anymore!  '\n",
    "tokens = [tokenize_tweet(word) for word in tweets];\n",
    "#clean_words\n",
    "clean_words = remove_punctuation(tokens);\n",
    "#clean_text\n",
    "clean_text = clean_text(clean_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocess(row_tweet):\n",
    "    return [(TwitterPreprocessor(row_tweet).fully_preprocess().text).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tweet(normalized_tweet):\n",
    "    vec=np.zeros((300))\n",
    "    tic=time.time()\n",
    "    for word in (normalized_tweet and model.vocab):\n",
    "        vec+=model[word]\n",
    "    toc=time.time()\n",
    "    print('time to vectorize one tweet!'+str((toc-tic)/60))\n",
    "    return preprocessing.normalize(vec.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_latest_tweets(tweets, model):\n",
    "    tweets.tweet= tweets.tweet.astype(str)\n",
    "    print('preprocessing starting...')\n",
    "    tic=time.time()\n",
    "    tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))\n",
    "    toc=time.time()\n",
    "    print(str((toc-tic)/60)+'minutes to normalize')\n",
    "    tic=time.time()\n",
    "    print('preprocessing finished!')\n",
    "    tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))\n",
    "    toc=time.time()\n",
    "    print(str((toc-tic)/60)+'minutes to vectorize')\n",
    "    print('tweets vectorizd!')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_user_input(user_input, model):\n",
    "    \n",
    "    normalized = tweet_preprocess(user_input)\n",
    "    word_vec = vectorize_tweet(normalized)\n",
    "    vectorized_input = {'raw_input': user_input, 'normalized': normalized, 'vector': word_vec}\n",
    "    return vectorized_input\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_tweets(vectorized_input, vectorized_tweets, topn):\n",
    "    vec_tweets=np.vstack(vectorized_tweets.vector.apply(lambda x: x.tolist()))\n",
    "    cos=model.cosine_similarities(vectorized_input['vector'].reshape(-1,), vec_tweets)\n",
    "    vectorized_tweets.loc[:,'similarity_score'] = cos #np.round(cos,10)\n",
    "    #decimals = pd.Series([8], index=['similarity_score'])\n",
    "    #vectorized_tweets.round(decimals)\n",
    "    vectorized_tweets = vectorized_tweets.sort_values(by='similarity_score', ascending=False)\n",
    "    return vectorized_tweets[0:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_input(user_input, time_limit, topic):\n",
    "\n",
    "    track_list=[k for k in topic.split(',')]\n",
    "    file_name = \"stream\"\n",
    "    if os.path.exists(data_dir+file_name+'.json'):\n",
    "        os.remove(data_dir+file_name+'.json')\n",
    "    latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)\n",
    "    if no_data:\n",
    "        return'There is no data with topic: '+topic+' in  '+ str(time_limit)+' seconds'\n",
    "    else:\n",
    "        latest_tweets = latest_tweets[0:1]\n",
    "        vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)\n",
    "        vectorized_user_input = vectorize_user_input(user_input, model)\n",
    "        #find the top topn= 10  similar tweets\n",
    "        recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)\n",
    "    return recommendations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloaded pretrained model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:  \n",
    "     credentials = json.load(file)\n",
    "# Instantiate an object\n",
    "python_tweets = Twython( credentials['CONSUMER_KEY'],  credentials['CONSUMER_SECRET'])\n",
    "auth = tweepy.OAuthHandler(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'])\n",
    "auth.set_access_token(credentials['ACCESS_TOKEN'], credentials['ACCESS_SECRET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streaming tweets...\n",
      "tweets obtained and now filtering...\n",
      "tweets are filtered                     time                                              tweet\n",
      "101  2019-05-27 19:15:08  @theSNP Just 505,786 vote for the SNP ,    348...\n",
      "99   2019-05-27 19:15:08  UK: 99% counted.\\n\\nRemain parties: 40.4%\\nHar...\n",
      "98   2019-05-27 19:15:08  If you want to make @UKLabour listen, take two...\n",
      "97   2019-05-27 19:15:08  A brilliant graphic from @SkyNews - clearly sh...\n",
      "88   2019-05-27 19:15:07  @blunted_james Policy has the option for a vot...\n",
      "80   2019-05-27 19:15:07  Brexit Derangement Syndrome has been on full d...\n",
      "81   2019-05-27 19:15:07  So in a nutshell\\n\\nEU 2014 - UKIP won\\nGE 201...\n",
      "83   2019-05-27 19:15:07  Nigel Farage today insisted he could WIN a Gen...\n",
      "84   2019-05-27 19:15:07  Whatever you think of his politics, this is a ...\n",
      "85   2019-05-27 19:15:07  Another thought. Brexit Party was clear about ...\n",
      "86   2019-05-27 19:15:07  @mireille_pouget @Maxwellsnp @AngusMacNeilSNP ...\n",
      "87   2019-05-27 19:15:07  @Independent Alyn Smith is absolutely right,of...\n",
      "89   2019-05-27 19:15:07  Sturgeon: strong case for new Scottish indepen...\n",
      "90   2019-05-27 19:15:07  *Before the EU election*\\n\\nLibDems: Labour is...\n",
      "91   2019-05-27 19:15:07  According to Electoral Calculus, if last night...\n",
      "92   2019-05-27 19:15:07  Be great to get some Brexit Party MEPs testify...\n",
      "93   2019-05-27 19:15:07  Just sliding in to observe that the Revoke Art...\n",
      "94   2019-05-27 19:15:07  Labour’s customs union Brexit is  a form of Br...\n",
      "95   2019-05-27 19:15:07  British-Asian MEP told to go home 'by Brexit P...\n",
      "73   2019-05-27 19:15:06  HYPOCRITICAL TRAITOR  @michaelgove says we nee...\n",
      "65   2019-05-27 19:15:06  @brexitparty_uk Excellent achievement, we need...\n",
      "66   2019-05-27 19:15:06  Tonight the Brexit Party wasn’t supported by a...\n",
      "67   2019-05-27 19:15:06  According to Electoral Calculus, if last night...\n",
      "69   2019-05-27 19:15:06  Farage isn't interested in negotiations - the ...\n",
      "70   2019-05-27 19:15:06  Change UK: Give us a break we've only been aro...\n",
      "71   2019-05-27 19:15:06  @this_mummy_runs @JeremyVineOn5 I have a genui...\n",
      "76   2019-05-27 19:15:06  My Brexit Party manifesto wish list\\n\\nWTO Bre...\n",
      "74   2019-05-27 19:15:06  POOR old Nigel Farage gets awfully tetchy *aga...\n",
      "77   2019-05-27 19:15:06  Does anyone else find that from time to time y...\n",
      "78   2019-05-27 19:15:06  Secret to Brexit Party success:\\nPublish no ma...\n",
      "..                   ...                                                ...\n",
      "22   2019-05-27 19:14:35  This is the message you won't find on any BBC ...\n",
      "23   2019-05-27 19:14:35  Isn't it wonderful what a good kick up the bum...\n",
      "25   2019-05-27 19:14:35  The same people who were claiming 80% ‘voted t...\n",
      "21   2019-05-27 19:14:35  The result of the #EUelections2019 has dispell...\n",
      "24   2019-05-27 19:14:35  I’m beginning to feel like a coked up chimp ba...\n",
      "27   2019-05-27 19:14:35  6,085,174 people signed the petition to revoke...\n",
      "26   2019-05-27 19:14:35  @Haggis_UK @campbellclaret The man is a con ma...\n",
      "28   2019-05-27 19:14:35  New Brexit Party MEP lives in...France. You co...\n",
      "29   2019-05-27 19:14:35  The BBC coverage ended up being surreal. \\nAn ...\n",
      "13   2019-05-27 19:14:34  After last week's EU elections, one thing is a...\n",
      "9    2019-05-27 19:14:34  If there’s one lesson we should learn from the...\n",
      "10   2019-05-27 19:14:34  @somwardner @Markbeaumontuk @Piers_Corbyn @Car...\n",
      "11   2019-05-27 19:14:34  The Brexit Party is doing all the electioneeri...\n",
      "12   2019-05-27 19:14:34  @hodgeflex @golfstar2010 @michaeljackson That’...\n",
      "14   2019-05-27 19:14:34  People saying we need a Brexit policy to bring...\n",
      "16   2019-05-27 19:14:34  The EU elections were the worst national elect...\n",
      "17   2019-05-27 19:14:34  Worst @GeneralBoles @Coldwar_Steve photomontag...\n",
      "18   2019-05-27 19:14:34  The great thing here is that from now on BBC P...\n",
      "19   2019-05-27 19:14:34  So far... in these elections the combined tall...\n",
      "20   2019-05-27 19:14:34  @_johnrlee @halfironmum @Herring1967 @Sportydo...\n",
      "5    2019-05-27 19:14:33  In the UK, the Brexit Party has a big lead. \\n...\n",
      "2    2019-05-27 19:14:33  Alastair Campbell believes this EU election re...\n",
      "3    2019-05-27 19:14:33  European election results: Remain did far bett...\n",
      "4    2019-05-27 19:14:33  \"There is now a majority to stop Brexit and re...\n",
      "1    2019-05-27 19:14:33  @Jasonngoose @AlexTheGuiri @AaronBastani How c...\n",
      "6    2019-05-27 19:14:33  #Cable is deluding himself if he thinks these ...\n",
      "7    2019-05-27 19:14:33  @EmmaKennedy in non-brexit news, my two year o...\n",
      "8    2019-05-27 19:14:33  First final results are now available for 10 c...\n",
      "0    2019-05-27 19:14:33  I’m v tired but the thing that strikes me abou...\n",
      "15                   NaN                                                NaN\n",
      "\n",
      "[96 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2019-05-27 19:15:08</td>\n",
       "      <td>@theSNP Just 505,786 vote for the SNP ,    348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2019-05-27 19:15:08</td>\n",
       "      <td>UK: 99% counted.\\n\\nRemain parties: 40.4%\\nHar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2019-05-27 19:15:08</td>\n",
       "      <td>If you want to make @UKLabour listen, take two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2019-05-27 19:15:08</td>\n",
       "      <td>A brilliant graphic from @SkyNews - clearly sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>@blunted_james Policy has the option for a vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Brexit Derangement Syndrome has been on full d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>So in a nutshell\\n\\nEU 2014 - UKIP won\\nGE 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Nigel Farage today insisted he could WIN a Gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Whatever you think of his politics, this is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Another thought. Brexit Party was clear about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>@mireille_pouget @Maxwellsnp @AngusMacNeilSNP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>@Independent Alyn Smith is absolutely right,of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Sturgeon: strong case for new Scottish indepen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>*Before the EU election*\\n\\nLibDems: Labour is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>According to Electoral Calculus, if last night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Be great to get some Brexit Party MEPs testify...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Just sliding in to observe that the Revoke Art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>Labour’s customs union Brexit is  a form of Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2019-05-27 19:15:07</td>\n",
       "      <td>British-Asian MEP told to go home 'by Brexit P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>HYPOCRITICAL TRAITOR  @michaelgove says we nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>@brexitparty_uk Excellent achievement, we need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>Tonight the Brexit Party wasn’t supported by a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>According to Electoral Calculus, if last night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>Farage isn't interested in negotiations - the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>Change UK: Give us a break we've only been aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>@this_mummy_runs @JeremyVineOn5 I have a genui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>My Brexit Party manifesto wish list\\n\\nWTO Bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>POOR old Nigel Farage gets awfully tetchy *aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>Does anyone else find that from time to time y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2019-05-27 19:15:06</td>\n",
       "      <td>Secret to Brexit Party success:\\nPublish no ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>This is the message you won't find on any BBC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>Isn't it wonderful what a good kick up the bum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>The same people who were claiming 80% ‘voted t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>The result of the #EUelections2019 has dispell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>I’m beginning to feel like a coked up chimp ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>6,085,174 people signed the petition to revoke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>@Haggis_UK @campbellclaret The man is a con ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>New Brexit Party MEP lives in...France. You co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-05-27 19:14:35</td>\n",
       "      <td>The BBC coverage ended up being surreal. \\nAn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>After last week's EU elections, one thing is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>If there’s one lesson we should learn from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>@somwardner @Markbeaumontuk @Piers_Corbyn @Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>The Brexit Party is doing all the electioneeri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>@hodgeflex @golfstar2010 @michaeljackson That’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>People saying we need a Brexit policy to bring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>The EU elections were the worst national elect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>Worst @GeneralBoles @Coldwar_Steve photomontag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>The great thing here is that from now on BBC P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>So far... in these elections the combined tall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-05-27 19:14:34</td>\n",
       "      <td>@_johnrlee @halfironmum @Herring1967 @Sportydo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>In the UK, the Brexit Party has a big lead. \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>Alastair Campbell believes this EU election re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>European election results: Remain did far bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>\"There is now a majority to stop Brexit and re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>@Jasonngoose @AlexTheGuiri @AaronBastani How c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>#Cable is deluding himself if he thinks these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>@EmmaKennedy in non-brexit news, my two year o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>First final results are now available for 10 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-27 19:14:33</td>\n",
       "      <td>I’m v tired but the thing that strikes me abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time                                              tweet\n",
       "101  2019-05-27 19:15:08  @theSNP Just 505,786 vote for the SNP ,    348...\n",
       "99   2019-05-27 19:15:08  UK: 99% counted.\\n\\nRemain parties: 40.4%\\nHar...\n",
       "98   2019-05-27 19:15:08  If you want to make @UKLabour listen, take two...\n",
       "97   2019-05-27 19:15:08  A brilliant graphic from @SkyNews - clearly sh...\n",
       "88   2019-05-27 19:15:07  @blunted_james Policy has the option for a vot...\n",
       "80   2019-05-27 19:15:07  Brexit Derangement Syndrome has been on full d...\n",
       "81   2019-05-27 19:15:07  So in a nutshell\\n\\nEU 2014 - UKIP won\\nGE 201...\n",
       "83   2019-05-27 19:15:07  Nigel Farage today insisted he could WIN a Gen...\n",
       "84   2019-05-27 19:15:07  Whatever you think of his politics, this is a ...\n",
       "85   2019-05-27 19:15:07  Another thought. Brexit Party was clear about ...\n",
       "86   2019-05-27 19:15:07  @mireille_pouget @Maxwellsnp @AngusMacNeilSNP ...\n",
       "87   2019-05-27 19:15:07  @Independent Alyn Smith is absolutely right,of...\n",
       "89   2019-05-27 19:15:07  Sturgeon: strong case for new Scottish indepen...\n",
       "90   2019-05-27 19:15:07  *Before the EU election*\\n\\nLibDems: Labour is...\n",
       "91   2019-05-27 19:15:07  According to Electoral Calculus, if last night...\n",
       "92   2019-05-27 19:15:07  Be great to get some Brexit Party MEPs testify...\n",
       "93   2019-05-27 19:15:07  Just sliding in to observe that the Revoke Art...\n",
       "94   2019-05-27 19:15:07  Labour’s customs union Brexit is  a form of Br...\n",
       "95   2019-05-27 19:15:07  British-Asian MEP told to go home 'by Brexit P...\n",
       "73   2019-05-27 19:15:06  HYPOCRITICAL TRAITOR  @michaelgove says we nee...\n",
       "65   2019-05-27 19:15:06  @brexitparty_uk Excellent achievement, we need...\n",
       "66   2019-05-27 19:15:06  Tonight the Brexit Party wasn’t supported by a...\n",
       "67   2019-05-27 19:15:06  According to Electoral Calculus, if last night...\n",
       "69   2019-05-27 19:15:06  Farage isn't interested in negotiations - the ...\n",
       "70   2019-05-27 19:15:06  Change UK: Give us a break we've only been aro...\n",
       "71   2019-05-27 19:15:06  @this_mummy_runs @JeremyVineOn5 I have a genui...\n",
       "76   2019-05-27 19:15:06  My Brexit Party manifesto wish list\\n\\nWTO Bre...\n",
       "74   2019-05-27 19:15:06  POOR old Nigel Farage gets awfully tetchy *aga...\n",
       "77   2019-05-27 19:15:06  Does anyone else find that from time to time y...\n",
       "78   2019-05-27 19:15:06  Secret to Brexit Party success:\\nPublish no ma...\n",
       "..                   ...                                                ...\n",
       "22   2019-05-27 19:14:35  This is the message you won't find on any BBC ...\n",
       "23   2019-05-27 19:14:35  Isn't it wonderful what a good kick up the bum...\n",
       "25   2019-05-27 19:14:35  The same people who were claiming 80% ‘voted t...\n",
       "21   2019-05-27 19:14:35  The result of the #EUelections2019 has dispell...\n",
       "24   2019-05-27 19:14:35  I’m beginning to feel like a coked up chimp ba...\n",
       "27   2019-05-27 19:14:35  6,085,174 people signed the petition to revoke...\n",
       "26   2019-05-27 19:14:35  @Haggis_UK @campbellclaret The man is a con ma...\n",
       "28   2019-05-27 19:14:35  New Brexit Party MEP lives in...France. You co...\n",
       "29   2019-05-27 19:14:35  The BBC coverage ended up being surreal. \\nAn ...\n",
       "13   2019-05-27 19:14:34  After last week's EU elections, one thing is a...\n",
       "9    2019-05-27 19:14:34  If there’s one lesson we should learn from the...\n",
       "10   2019-05-27 19:14:34  @somwardner @Markbeaumontuk @Piers_Corbyn @Car...\n",
       "11   2019-05-27 19:14:34  The Brexit Party is doing all the electioneeri...\n",
       "12   2019-05-27 19:14:34  @hodgeflex @golfstar2010 @michaeljackson That’...\n",
       "14   2019-05-27 19:14:34  People saying we need a Brexit policy to bring...\n",
       "16   2019-05-27 19:14:34  The EU elections were the worst national elect...\n",
       "17   2019-05-27 19:14:34  Worst @GeneralBoles @Coldwar_Steve photomontag...\n",
       "18   2019-05-27 19:14:34  The great thing here is that from now on BBC P...\n",
       "19   2019-05-27 19:14:34  So far... in these elections the combined tall...\n",
       "20   2019-05-27 19:14:34  @_johnrlee @halfironmum @Herring1967 @Sportydo...\n",
       "5    2019-05-27 19:14:33  In the UK, the Brexit Party has a big lead. \\n...\n",
       "2    2019-05-27 19:14:33  Alastair Campbell believes this EU election re...\n",
       "3    2019-05-27 19:14:33  European election results: Remain did far bett...\n",
       "4    2019-05-27 19:14:33  \"There is now a majority to stop Brexit and re...\n",
       "1    2019-05-27 19:14:33  @Jasonngoose @AlexTheGuiri @AaronBastani How c...\n",
       "6    2019-05-27 19:14:33  #Cable is deluding himself if he thinks these ...\n",
       "7    2019-05-27 19:14:33  @EmmaKennedy in non-brexit news, my two year o...\n",
       "8    2019-05-27 19:14:33  First final results are now available for 10 c...\n",
       "0    2019-05-27 19:14:33  I’m v tired but the thing that strikes me abou...\n",
       "15                   NaN                                                NaN\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Users/shahla/Dropbox/SharpestMinds/stream/data/'\n",
    "user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'\n",
    "topic = 'brexit' #it can be a list of topics,  comman means 'or'\n",
    "time_limit=5\n",
    "user_input = 'Canadian retaliatory tariffs lifted as U.S. kills steel aluminum penalties'\n",
    "#recommendations = process_user_input(user_input, time_limit, topic)\n",
    "track_list=[k for k in topic.split(',')]\n",
    "file_name = \"stream\"\n",
    "\n",
    "latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)\n",
    "\n",
    "#latest_tweets = latest_tweets[0:1]\n",
    "# vectorized_tweets = vectorize_latest_tweets(latest_tweets, model)\n",
    "# vectorized_user_input = vectorize_user_input(user_input, model)\n",
    "# #find the top topn= 10  similar tweets\n",
    "# recommendations = find_most_similar_tweets(vectorized_user_input, vectorized_tweets,topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2019-05-27 19:15:08</td>\n",
       "      <td>@theSNP Just 505,786 vote for the SNP ,    348...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time                                              tweet\n",
       "101  2019-05-27 19:15:08  @theSNP Just 505,786 vote for the SNP ,    348..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = latest_tweets.copy()\n",
    "tweets= tweets[0:1]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing starting...\n",
      "remove_hyphens3.0994415283203125e-06\n",
      "remove_urls5.4836273193359375e-05\n",
      "remove_mentions3.600120544433594e-05\n",
      "remove_hashtags0.00011491775512695312\n",
      "remove_twitter_reserved4.482269287109375e-05\n",
      "remove_single_letter3.504753112792969e-05\n",
      "remove_stop0.0015201568603515625\n",
      "remove_numbers1.3113021850585938e-05\n",
      "str_html0.0010502338409423828\n",
      "remove accented2.288818359375e-05\n",
      "expand_contractions0.0005340576171875\n",
      "lemmatize1.7759349346160889\n",
      "remove special5.3882598876953125e-05\n",
      "remove_stop0.0008461475372314453\n",
      "remove_blank1.5974044799804688e-05\n",
      "lower9.5367431640625e-07\n",
      "0.03087511459986369minutes to normalize\n"
     ]
    }
   ],
   "source": [
    "tweets.tweet= tweets.tweet.astype(str)\n",
    "print('preprocessing starting...')\n",
    "tic=time.time()\n",
    "tweets.loc[:,'normalized'] = tweets.tweet.apply(lambda row_tweet:tweet_preprocess(row_tweet))\n",
    "toc=time.time()\n",
    "print(str((toc-tic)/60)+'minutes to normalize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tic=time.time()\n",
    "print('preprocessing finished!')\n",
    "tweets.loc[:,'vector'] = tweets.normalized.apply(lambda tweet:vectorize_tweet(tweet))\n",
    "toc=time.time()\n",
    "print(str((toc-tic)/60)+'minutes to vectorize')\n",
    "print('tweets vectorizd!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   time  \\\n",
      "49  2019-05-27 19:06:08   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                       tweet  \\\n",
      "49  Labour bigwigs say they now need to commit to a second referendum. Yet in places such as Sunderland it lost 20 per cent of its vote share while the Brexit Party took an extraordinary 40 per cent of the vote. Apparently Labour’s traditional heartlands don’t matter to them anymore!   \n",
      "\n",
      "                                                                                                                                                                                                                           normalized  \\\n",
      "49  [[labour, bigwig, say, need, commit, second, referendum, yet, place, sunderland, lose, per, cent, vote, share, brexit, party, take, extraordinary, per, cent, vote, apparently, labour, traditional, heartland, matter, anymore]]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                         vector  \\\n",
      "49  [[-0.00032382228481594847, -0.0014532627079806516, -0.015030095810103974, 0.04513634569000594, 0.0027960322663428323, -0.06937477717077793, 0.012495866273806368, -0.06008408953007944, 0.03607476254107687, 0.06574652608953467, 0.0018089693189037462, -0.0025177909547333018, -0.03594986706811807, 0...   \n",
      "\n",
      "    similarity_score  \n",
      "49               1.0  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 300):\n",
    "    print (recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6017679], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cosine_similarities(model[''],model['Iran'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streaming tweets...\n",
      "tweets obtained and now filtering...\n",
      "tweets are filtered                    time                                              tweet\n",
      "80  2019-05-27 15:33:34  Oh dear According to Electoral Calculus, if la...\n",
      "79  2019-05-27 15:33:34  I wonder why Anas might be saying something li...\n",
      "78  2019-05-27 15:33:34  EU election results as of right now:\\n\\nNigel ...\n",
      "70  2019-05-27 15:33:33                                   Which September?\n",
      "64  2019-05-27 15:33:33  The Brexit Party Could be Stopped on June 6th ...\n",
      "65  2019-05-27 15:33:33  4. Brexit: UK result shows the country is spli...\n",
      "66  2019-05-27 15:33:33  @SKinnock I had to pinch myself but I agree wi...\n",
      "67  2019-05-27 15:33:33  So it’s another woeful performance for @Nigel_...\n",
      "69  2019-05-27 15:33:33  So far... in these elections the combined tall...\n",
      "71  2019-05-27 15:33:33  POOR old Nigel Farage gets awfully tetchy *aga...\n",
      "72  2019-05-27 15:33:33  Fuck it! No, sorry I mean Bollocks to Brexit! ...\n",
      "73  2019-05-27 15:33:33  Before twitter whips itself into a frenzy befo...\n",
      "74  2019-05-27 15:33:33  This how you take your country back, I agree w...\n",
      "75  2019-05-27 15:33:33  @Channel4News You should be careful with the u...\n",
      "76  2019-05-27 15:33:33                                     Labour wankers\n",
      "77  2019-05-27 15:33:33  I can’t believe people have ACTUALLY voted for...\n",
      "59  2019-05-27 15:33:32  From my Euro-election post vote poll: 53% of 2...\n",
      "56  2019-05-27 15:33:32  Current popular vote has Green/LD/CHUK at 5.4m...\n",
      "57  2019-05-27 15:33:32  Labour's position was clear to me because I he...\n",
      "58  2019-05-27 15:33:32  SMR | THE FACTS SIMPLY CANNOT LIE | EVEN IF TH...\n",
      "60  2019-05-27 15:33:32  @DPJHodges Wales voted Brexit. \\n\\nSince the L...\n",
      "62  2019-05-27 15:33:32  The \"fear of Corbyn\" excuse is a blatant lie f...\n",
      "63  2019-05-27 15:33:32  This is not a high point for The Brexit Party ...\n",
      "50  2019-05-27 15:33:31  Q:\\nBrexit Party got 5.25million votes, from 1...\n",
      "45  2019-05-27 15:33:31  @sinnfeinireland @MaryLouMcDonald @M_AndersonS...\n",
      "46  2019-05-27 15:33:31  @lorraineSW1 @JeSuisDog yes too true. i bet de...\n",
      "47  2019-05-27 15:33:31  The 5.24m votes the Brexit Party received is o...\n",
      "49  2019-05-27 15:33:31  The Brexit party was meant to have a formal pr...\n",
      "51  2019-05-27 15:33:31  @cheekybunbuns I thought the UK did pretty goo...\n",
      "52  2019-05-27 15:33:31  The @UKLabour conference end 5 weeks and a day...\n",
      "..                  ...                                                ...\n",
      "28  2019-05-27 14:57:29  Alastair Campbell believes this EU election re...\n",
      "20  2019-05-27 14:57:28  Nigel Farage's Brexit party will be the LARGES...\n",
      "23  2019-05-27 14:57:28  EU election results as of right now:\\n\\nNigel ...\n",
      "19  2019-05-27 14:57:28  My dear @AnnaSobriety ....... didn't you say o...\n",
      "21  2019-05-27 14:57:28  @WCullmac ....and tell everyone who is fed up ...\n",
      "22  2019-05-27 14:57:28  To recap, Farage increasing his MEPs by 4 (fou...\n",
      "24  2019-05-27 14:57:28  @paulmasonnews @SarahLudford EU elections:\\nRe...\n",
      "25  2019-05-27 14:57:28  United Kingdom, Great Britain result map:\\n\\n(...\n",
      "26  2019-05-27 14:57:28  Baffling that we have Labour spokespeople out ...\n",
      "27  2019-05-27 14:57:28  Recognise this, Australia? #auspol https://t.c...\n",
      "13  2019-05-27 14:57:27  Billionaire Richard Tice elected as an MEP for...\n",
      "8   2019-05-27 14:57:27  6,085,174 people signed the petition to revoke...\n",
      "9   2019-05-27 14:57:27  A new Prime Minister should say to the EU we w...\n",
      "10  2019-05-27 14:57:27  BBC misrepresenting the issue. When you add up...\n",
      "11  2019-05-27 14:57:27  @OurCymru @Plaid_Cymru @Adamprice Tell me one ...\n",
      "12  2019-05-27 14:57:27  The UK is as divided as ever on #Brexit.\\n\\nWh...\n",
      "14  2019-05-27 14:57:27  @lionelbarber @MikeMol1982 Are you mad?\\n\\nI’l...\n",
      "15  2019-05-27 14:57:27  Galloway trying to stand for the Brexit party\\...\n",
      "16  2019-05-27 14:57:27  It does seem like an awful lot of Brexit voter...\n",
      "17  2019-05-27 14:57:27  @boldbigboy71 @sandra_godfrey It tells me two ...\n",
      "18  2019-05-27 14:57:27  UK: 99% counted.\\n\\nRemain parties: 40.4%\\nHar...\n",
      "4   2019-05-27 14:57:26  The parties of Le Pen, Farage and Salvini have...\n",
      "2   2019-05-27 14:57:26  You do realise Tories and Labour are leave rig...\n",
      "3   2019-05-27 14:57:26  Gosh how did Mrs T win three massive victories...\n",
      "1   2019-05-27 14:57:26  Oh my, today of all days, you couldn't have pi...\n",
      "5   2019-05-27 14:57:26  People saying we need a Brexit policy to bring...\n",
      "6   2019-05-27 14:57:26  This petition got more votes than the Brexit P...\n",
      "7   2019-05-27 14:57:26  @Nigel_Farage Start with Peterborough 6 June. ...\n",
      "0   2019-05-27 14:57:26  @afneil @SocialM85897394 The British People de...\n",
      "37                  NaN                                                NaN\n",
      "\n",
      "[78 rows x 2 columns]\n",
      "preprocessing starting...\n",
      "0.01889114777247111minutes to normalize\n",
      "preprocessing finished!\n",
      "1.089441434542338minutes to vectorize\n",
      "tweets vectorizd!\n"
     ]
    }
   ],
   "source": [
    "track_list=[k for k in topic.split(',')]\n",
    "latest_tweets, no_data = get_latest_tweets(data_dir, auth, time_limit, track_list)\n",
    "latest_tweets = latest_tweets[0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
