{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackabuse.com/accessing-the-twitter-api-with-python/\n",
    "# to save my twitter_credentials.json\n",
    "import json\n",
    "\n",
    "## Enter your keys/secrets as strings in the following fields\n",
    "#credentials = {}  \n",
    "#credentials['CONSUMER_KEY'] = 'lrPS9OHXHvE035nktii81w5BF'\n",
    "#credentials['CONSUMER_SECRET'] = 'pF76sVbMXkVPAZZ4eqaDLsQBMeFKCYctvJoPxlamqje8WHq8n7' \n",
    "#credentials['ACCESS_TOKEN'] = '757203324970102784-3El1NyAOlq0oulCAzuDfj92Jp0L0fiy'\n",
    "#credentials['ACCESS_SECRET'] = 'EEpo2ZfpRINSDoh5amp8Jw8Zj985KVRutIwSLKTrOt2lZ'\n",
    "\n",
    "## Save the credentials object to file\n",
    "#with open(\"twitter_credentials.json\", \"w\") as file:  \n",
    "#    json.dump(credentials, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twython class\n",
    "from twython import Twython  \n",
    "import json\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:  \n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our query\n",
    "#We'll use only four arguments in the query: \n",
    "#q, result_type, count and lang, respectively for the search keyword, type, count, and language of results.\n",
    "#for more info see https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html\n",
    "query = {'q': 'trump',  \n",
    "        'result_type': 'popular',#mixed recent popular\n",
    "        'count': 200,\n",
    "        'lang': 'en',\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Search tweets\n",
    "dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  \n",
    "for status in python_tweets.search(**query)['statuses']:  \n",
    "    dict_['user'].append(status['user']['screen_name'])\n",
    "    dict_['date'].append(status['created_at'])\n",
    "    dict_['text'].append(status['text'])\n",
    "    dict_['favorite_count'].append(status['favorite_count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Structure data in a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(dict_)  \n",
    "df.sort_values(by='favorite_count', inplace=True, ascending=False)  \n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing\n",
    "Depending on your downstream task, cleaning and pre-processing text can involve several different components.  Here are a few important components of Natural Language Processing (NLP) pipelines.\n",
    "\n",
    "1.  Removing tags: unnecessary content like HTML tags\n",
    "\n",
    "2.  Removing accented characters: other languages such as French, convert ASCII\n",
    "\n",
    "3. Removing special characters: adds noise to text, use simple regular expressions (regexes)\n",
    "\n",
    "4.  Stemming and lemmatization: Stemming remove prefixes and suffixes of word stems (i.e. root words), ex. WATCH is the root stem of WATCHES, WATCHING, and WATCHE.  Lemmatization similar but lexicographically correct word (present in the dictionary).\n",
    "\n",
    "5.  Expanding contractions: helps text standardization, ex. do not to don’t and I would to I’d\n",
    "\n",
    "6.  Removing stopwords: Words without meaningful significance (ex. a, an, the, and) but high frequency.\n",
    "\n",
    "Additional pre-processing: tokenization, removing extra whitespaces,  lower casing and more advanced operations like spelling corrections, grammatical error corrections, removing repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#%run /Users/shahla/Dropbox/MLCourse/Day1/3_sentiment/contractions.py\n",
    "from contractions import CONTRACTION_MAP #contraction is a  pyhton file in my local computer\n",
    "import unicodedata\n",
    "\n",
    "#python3 -m spacy download en #install it in the terminal\n",
    "\n",
    "nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog', 'brown fox quick blue dog lazy',\n",
       "       'sky blue sky beautiful today', 'dog lazy brown fox quick'],\n",
       "      dtype='<U30')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We asked @AdamSchiff to see Michael Cohen’s testimony in front of his committee. No answer!\\n\\nBut Schiff wants the A… https://t.co/fNUkwp6xCV'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['asked adamschiff see michael cohens testimony front committee . answer ! schiff wants … https :// . co / fnukwp6xcv',\n",
       "       'latest michael cohen request end debate motive keep jail give ne … https :// . co / i99lqosseb',\n",
       "       'michael cohens attorneys told lawmakers letter cohen discovered substantial files hard dri https :// . co / unyaegcic6',\n",
       "       'dems first big hearingmichael cohen — ’ going prison lying congress . testifies , lies agai … https :// . co / nbmxnuptww',\n",
       "       'cnn michael cohen wants stay jail tells congress documents offer assistance investigators',\n",
       "       'donald trumps day far - michael cohen found old hard drive full evidence trump - capital … https :// . co / zwwqyssuq6',\n",
       "       'two michael cohen facts 1 sought pardon potus . ( also lied congress , oath ) 2 ) … https :// . co / 55ggph9jc6',\n",
       "       'cohens attorneys told hill letter thursday cohen discovered substantial files hard drive https :// . co / noaw8lqa9y',\n",
       "       'michael cohen gift keeps giving new documents cohen says trump \" instructed \" lie … https :// . co / oarwlsz6mb',\n",
       "       'michael cohens attorney lanny davis , says ltr hse cmtes \" cohen located several documents believ … https :// . co / hy1lqq3k0u'],\n",
       "      dtype='<U127')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_normalizer import normalize_corpus\n",
    "x=normalize_corpus(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his latest Michael Cohen request should end any debate about his motive. “Keep me out of jail so I '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.text[0][1:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can give you NE… https://t.co/I99lqoSsEb'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0][100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'michael cohen attorney lanny davi say ltr hse cmte cohen locate several document believ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We asked @AdamSchiff to see Michael Cohen’s testimony in front of his committee. No answer!\\n\\nBut Schiff wants the A… https://t.co/fNUkwp6xCV'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two Michael Cohen facts:\\n\\n1) He sought a pardon from POTUS. (And also lied to Congress about this, under oath)\\n2) S… https://t.co/55ggpH9jc6'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
